{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DC-Biking State Usage Prediction Based On Weather\n",
    "\n",
    "### 15688 Team Project \n",
    "\n",
    "Bowen Yang, I-Huei Huang, Gilbert Gao"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Motivation\n",
    "Traffic flow prediction can be difficult because of its randomness natural. However, if we can predict the bike requirements, it will be helpful to schedule bikes for a bike rental system. In our team project, we will leverage the dataset provided by [Capital Bikeshare](https://www.capitalbikeshare.com/) system located at Washington D.C. as well as the historical weather data queried from the [Weather Underground history API](https://www.wunderground.com/weather/api/d/docs?d=data/history) .\n",
    "\n",
    "By combining these data together, we plan to build a machine learning model that take historical bike using data and weather information as input and make prediction on total number of bikes required in Washington D.C. based on features of weather and the location of the bike station."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Report Contents\n",
    "* [Prerequisite](#Prerequisite)\n",
    "* [Phase 1: Clean out the history bike data](#Phase 1: Clean out the history bike data)\n",
    "* [Phase 2: Collect weather information -- weather underground API](#Phase 2: Collect weather information -- weather underground API) \n",
    "* [Further work](#Further work)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prerequisite\n",
    "Download [history data from 2011 to 2015](https://s3.amazonaws.com/capitalbikeshare-data/index.html) from [Capital BikeShare](https://www.capitalbikeshare.com/trip-history-data).\n",
    "\n",
    "Since the dataset is located in s3 storage, after configure the AWS Command Line Interface, we can download the whole dataset in a single line:\n",
    "\n",
    "`$ aws s3 cp s3://capitalbikeshare-data . --recursive`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Phase 1: Clean out the history bike data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we can start to clean out the history bike data. \n",
    "\n",
    "First we need to parse the station information from the XML file downloaded in the website. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import io, time, json\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import glob\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "soup = BeautifulSoup(open(\"stations.xml\"), \"lxml\")\n",
    "station_list = soup.find_all('station')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `station_list` is where we save all the station tag informations. We can print one of them to see what's inside to make ourselves clear about how to parse it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<station><id>1</id><name>Eads St &amp; 15th St S</name><terminalname>31000</terminalname><lastcommwithserver>1478040900207</lastcommwithserver><lat>38.858971</lat><long>-77.05323</long><installed>true</installed><locked>false</locked><installdate>0</installdate><removaldate></removaldate><temporary>false</temporary><public>true</public><nbbikes>6</nbbikes><nbemptydocks>8</nbemptydocks><latestupdatetime>1478040899170</latestupdatetime></station>\n"
     ]
    }
   ],
   "source": [
    "print station_list[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Then now we can generate a dataframe to get the needed information and save into a csv file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "result = []\n",
    "for each_tag in station_list:\n",
    "    station_dict = {}\n",
    "    station_dict['id'] = each_tag.id.string\n",
    "    station_dict['name'] = each_tag.find('name').string\n",
    "    station_dict['terminalname'] = each_tag.terminalname.string\n",
    "    station_dict['lat'] = float(each_tag.lat.string)\n",
    "    station_dict['long'] = float(each_tag.find('long').string)\n",
    "    result.append(station_dict)\n",
    "station_df = pd.DataFrame.from_dict(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  id        lat       long                                        name  \\\n",
      "0  1  38.858971 -77.053230                         Eads St & 15th St S   \n",
      "1  2  38.857250 -77.053320                             18th & Eads St.   \n",
      "2  3  38.856425 -77.049232                           20th & Crystal Dr   \n",
      "3  4  38.860170 -77.049593                           15th & Crystal Dr   \n",
      "4  5  38.857866 -77.059490  Aurora Hills Community Ctr/18th & Hayes St   \n",
      "\n",
      "  terminalname  \n",
      "0        31000  \n",
      "1        31001  \n",
      "2        31002  \n",
      "3        31003  \n",
      "4        31004  \n"
     ]
    }
   ],
   "source": [
    "print station_df.head()\n",
    "station_df.to_csv('station.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we move to the raw bike data to parse the useful information. \n",
    "First, we need the help from `glob` to get all the data files:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "all_historical = glob.glob(\"*-cabi-trip-history-data.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For our analysis, we only care about when and where the bike riders start their travel. Therefore we filter out all the unuseful information for us to simplify the parsing data size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "historical_df_list = []\n",
    "for each_file in all_historical:\n",
    "    temp_df = pd.read_csv(each_file)\n",
    "    temp_df.columns = [each.lower() for each in temp_df.columns]\n",
    "    if 'start station number' in temp_df.columns:\n",
    "        # This is a updated version of dataset\n",
    "        temp_df = temp_df[['start date','start station','start station number']]\n",
    "        historical_df_list.append(temp_df)\n",
    "    else:\n",
    "        # older ones \n",
    "        temp_df = temp_df[['start date','start station']]\n",
    "        historical_df_list.append(temp_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The original data comes with multiple date format. To organize and for future merging usage, we need to parse the dates to have same output format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Parse the dates\n",
    "for each_df in historical_df_list:\n",
    "    each_df['start date'] =  pd.to_datetime(each_df['start date'], infer_datetime_format = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The other messy part is that some of the station name does not only contains the name, but also the changed station name (represented by '[]') and the station terminal name (represented by the '()'). So a parsing using regular expression is needed. \n",
    "\n",
    "The changed station is saved for future use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "station_change = []\n",
    "for each_df in historical_df_list:\n",
    "    # Filter out the changed names\n",
    "    extract_start = each_df['start station'].str.extract('(?P<extracted>[^\\[\\]]+) *(?P<change>\\[[^\\[\\]]+\\])? *', expand=False)\n",
    "    new_start = extract_start.extracted.apply(lambda x: str(x).rstrip())\n",
    "    each_df['start station'] = new_start\n",
    "    # Then get the station change\n",
    "    if extract_start.shape[1] > 1:\n",
    "        station_change.append(extract_start.dropna(how='any').drop_duplicates())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we started to merge the station information with the historical travel data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index([u'id', u'lat', u'long', u'name', u'terminalname'], dtype='object')\n",
      "lat             float64\n",
      "long            float64\n",
      "name             object\n",
      "terminalname      int64\n",
      "dtype: object\n",
      "start date              datetime64[ns]\n",
      "start station                   object\n",
      "start station number             int64\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "station_df = pd.read_csv('station.csv')\n",
    "print station_df.columns\n",
    "useful_station_df = station_df[[u'lat', u'long', u'name', u'terminalname']]\n",
    "print useful_station_df.dtypes\n",
    "print historical_df_list[-1].dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data sets are not consistant for all these years. Some of the "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "combined = []\n",
    "# Find the useful information from the station_df\n",
    "for each_df in historical_df_list:\n",
    "    if \"start station number\" in each_df.columns:\n",
    "        # merge on the numbers\n",
    "        merged = each_df.merge(useful_station_df, left_on='start station number', right_on='terminalname', how='inner')\n",
    "        combined.append(merged[['start date','start station','terminalname', 'lat', 'long']])\n",
    "    else:\n",
    "        # have to merge on the names:\n",
    "        merged = each_df.merge(useful_station_df, left_on='start station', right_on='name', how='inner')\n",
    "        combined.append(merged[['start date','start station','terminalname', 'lat', 'long']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After merging, we can `concat` the list of dataframes and generate a master dataset for future usage. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(11815281, 5)\n"
     ]
    }
   ],
   "source": [
    "total_df = pd.concat(combined)\n",
    "print total_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "total_df.to_csv(\"merged_data.csv\", index = False, date_format =\"%m/%d/%Y %H:%M\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So now we have a historical travel dataset ready for merge and future manipulation. \n",
    "\n",
    "We are curious about how the weather information have influence the usage of the bike trips. Since we now have the travel data, we can move to the weather data retrieving. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Phase 2: Collect weather information -- weather underground API \n",
    "We collect historical weather information from 2011 to 2015 by calling history weather API on [Weather Underground](https://www.wunderground.com/weather/api/d/docs?d=data/history).\n",
    "After downloading [history data from 2011 to 2015](https://s3.amazonaws.com/capitalbikeshare-data/index.html) from [Capital BikeShare](https://www.capitalbikeshare.com/trip-history-data), and applied API form wunderground.com. We extract the start date of each trip and call the API:\n",
    "    __http://api.wunderground.com/api/{APIkey}/history_YYYYMMDD/q/DC.json__\n",
    "to get the weather information in that day. And then, we parse the json format to get the weather at an exact hour in that day.\n",
    "The following is the python script we used in this process:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we include the packages we need:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "import csv\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we extract the weather information from 2011 to 2015 by calling weather undergound history API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "key = 'your_api_key'\n",
    "years = ['2011', '2012', '2013', '2014', '2015']\n",
    "seasons = ['Q1', 'Q2', 'Q3', 'Q4']\n",
    "\n",
    "for y in years:\n",
    "    for s in seasons:\n",
    "        # create log file\n",
    "        logfile = \"weather/log_\" + y + \"_\" + s + \".txt\"\n",
    "        log = open(logfile, \"wb\")\n",
    "        \n",
    "        # create read and write file\n",
    "        rfile = 'data/' + y + '-' + s + '-cabi-trip-history-data.csv'\n",
    "        wfile = 'weather/' + rfile.split('/')[1].split('.')[0] +  '-weather.csv'\n",
    "        csvrfile = open(rfile, 'rb')\n",
    "        csvwfile = open(wfile, 'wb')\n",
    "        \n",
    "        # read csv file\n",
    "        reader = csv.DictReader(csvrfile)\n",
    "        rfieldnames = reader.fieldnames\n",
    "        \n",
    "        # write csv file\n",
    "        wfieldnames = ['Start date', 'Weather type', 'Temperature', 'Humidity', 'Wind speed']\n",
    "        writer = csv.DictWriter(csvwfile, fieldnames=wfieldnames)\n",
    "        writer.writeheader()\n",
    "        \n",
    "        # store the time we have already visited to increase performance\n",
    "        visit = set()\n",
    "        for row in reader:\n",
    "            d = {}\n",
    "            # check the format of the date is valid: MM/DD/YYYY hh:mm\n",
    "            if '/' not in row['Start date']:\n",
    "                date_and_time = row['Start date'].split()\n",
    "                mdate = date_and_time[0]\n",
    "                num = mdate.split('-')\n",
    "                mdate = num[1] + '/' + num[2] + '/' + num[0]\n",
    "                mtime = date_and_time[1]\n",
    "                row['Start date'] = mdate + ' ' + mtime\n",
    "                \n",
    "            # extract the date we need to call weather API    \n",
    "            t = row['Start date']\n",
    "            t = t.split()\n",
    "\n",
    "            try:\n",
    "                date = t[0].split('/') \n",
    "                year = int(date[2])\n",
    "                mon = int(date[0])\n",
    "                mday = int(date[1])\n",
    "                date = str(year) + '%02d' % mon + '%02d' % mday\n",
    "                t = t[1].split(':')\n",
    "                hour = int(t[0])\n",
    "                key = date + \":\" + str(hour)\n",
    "                if key in visit:\n",
    "                    continue\n",
    "                visit.add(key)\n",
    "            except:\n",
    "                print row['Start date']\n",
    "                continue\n",
    "            \n",
    "            # begin to call weather API\n",
    "            get = False\n",
    "            while not get:\n",
    "                try:\n",
    "                    url = 'http://api.wunderground.com/api/' + key + '/history_' + date + '/q/DC.json'\n",
    "                    r = requests.get(url)\n",
    "                    parsed_json = json.loads(r.text)\n",
    "                    observations = parsed_json[\"history\"][\"observations\"]\n",
    "                    get = True\n",
    "                except:\n",
    "                    time.sleep(100)\n",
    "        \n",
    "            # find the weather information at the exact hour we need\n",
    "            for info in observations:\n",
    "\n",
    "                if int(info[\"date\"][\"year\"]) == year and int(info[\"date\"][\"mon\"]) == mon and int(\n",
    "                        info[\"date\"][\"mday\"]) == mday and int(info[\"date\"][\"hour\"]) == hour:\n",
    "                    try:\n",
    "                        d['Weather type'] = info[\"icon\"]\n",
    "                        d['Temperature'] = info[\"tempm\"]\n",
    "                        d['Humidity'] = float(info[\"hum\"]) / 100\n",
    "                        d['Wind speed'] = info[\"wspdm\"]\n",
    "                    except:\n",
    "                        print year, mon, mday, hour\n",
    "                        print info[\"icon\"]\n",
    "                        print info[\"tempm\"]\n",
    "                        print info[\"hum\"]\n",
    "                        print info[\"wspdm\"]\n",
    "                    break\n",
    "                \n",
    "            d['Start date'] = row['Start date'].split(':')[0]\n",
    "            \n",
    "            # If the data is missing, we discard the record, else write it to output file\n",
    "            if len(d) != 5:\n",
    "                print date + \":\" + str(hour)\n",
    "                log.write(date + \":\" + str(hour)+ \"\\n\")\n",
    "            else:\n",
    "                writer.writerow(d)\n",
    "                \n",
    "        print y + \"-\" + s + \" finished\"\n",
    "        log.write(y + \"-\" + s + \" finished\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The weather information collected is with the following format:\n",
    "\n",
    "|  Start date     |  Weather Type\t|   Temperature   |    Humidity     |   Wind Speed    |\n",
    "|:---------------:|:---------------:|:---------------:|:---------------:|:---------------:|\n",
    "| 01/01/2014 0    |\t    clear\t    |      1.1\t      |       0.46      |   0             |\n",
    "| 01/01/2014 1\t  |    partlycloudy |      -1.1\t      |       0.61      |   13            |\n",
    "| 01/01/2014 2\t  |    partlycloudy |      -0.6\t      |       0.57      |   5.6           |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
